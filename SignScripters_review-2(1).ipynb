{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafca354-8c24-4043-8305-c80f21f074f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076aa4db-a12b-499d-a919-3bdc068e259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Divya sri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Create a more complex CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0657b841-350e-44b0-8a5c-9e1709d7af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb51f657-76fa-4f74-9474-ee8f5acf98f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5020 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "#Assuming we have our own dataset for training\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "         r'D:\\bcd\\archive',\n",
    "         target_size=(150, 150),\n",
    "         batch_size=20,\n",
    "         class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11bfbcd9-6d60-4bb5-9818-052bc7bf9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "478a6c0c-2a76-4119-8aac-728668a73efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('small_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdc1d631-f02e-4f33-96d7-a8249325a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the camera to capture an image\n",
    "#camera = cv2.VideoCapture(0)\n",
    "#eturn_value, image = camera.read()\n",
    "#cv2.imwrite('C:/asl/asl_alphabet_test/0_test.jpeg', image)\n",
    "#del(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749b63cb-27a8-4468-8ae9-135e737496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess the image for the model\n",
    "img_path = r'D:\\bcd\\archive\\c'\n",
    "#img = image.load_img(img_path, target_size=(150, 150))\n",
    "#x = image.img_to_array(img)\n",
    "#x = np.expand_dims(x, axis=0)\n",
    "#x = x / 255.0  # Normalize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf6fdace-3bd7-476c-aff6-f370614f342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bb2c91f-694b-4aa9-a93c-c924f03bd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img, (150, 150))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img / 255.0  # Normalize the image\n",
    "    else:\n",
    "        print(f\"Failed to load image at {img_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7e701dc-06b6-4f77-976f-f9a949000446",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [r'D:\\bcd\\archive\\c\\hand2_c_right_seg_2_cropped.jpeg',\n",
    "               r'D:\\bcd\\archive\\c\\hand5_c_dif_seg_3_cropped.jpeg',\n",
    "               r'D:\\bcd\\archive\\c\\hand5_c_dif_seg_5_cropped.jpeg']\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ce25fe2-2893-4770-9283-1c46f15df5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Image D:\\bcd\\archive\\c\\hand2_c_right_seg_2_cropped.jpeg contains an object as per the trained model.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Image D:\\bcd\\archive\\c\\hand5_c_dif_seg_3_cropped.jpeg contains an object as per the trained model.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Image D:\\bcd\\archive\\c\\hand5_c_dif_seg_5_cropped.jpeg contains an object as per the trained model.\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_images = len(test_images)\n",
    "for img_path in test_images:\n",
    "    preprocessed_img = preprocess_image(img_path)\n",
    "    if preprocessed_img is not None:\n",
    "        prediction = model.predict(preprocessed_img)\n",
    "        if prediction[0][0] > 0.2:\n",
    "            print(f\"Image {img_path} contains an object as per the trained model.\")\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            print(f\"Image {img_path} does not contain an object as per the trained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7687600-15db-4ae4-8283-96b657ca8a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image D:\\bcd\\archive\\c\\hand5_c_dif_seg_5_cropped.jpeg contains an object as per the trained model.\n"
     ]
    }
   ],
   "source": [
    "if prediction[0][0] < 0.5:\n",
    "        print(f\"Image {img_path} contains an object as per the trained model.\")\n",
    "        correct_predictions += 1\n",
    "else:\n",
    "        print(f\"Image {img_path} does not contain an object as per the trained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0654415-a8fa-4084-9d3a-bf16ea0585da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 133.33333333333331%\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print accuracy\n",
    "accuracy = correct_predictions / total_images * 100\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a3380-0573-4d28-bbea-6cafc938f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Capture test image from camera\n",
    "def capture_test_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('Test Image', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            predicted_text = predict_asl_image(frame)\n",
    "            print(f\"Predicted ASL text: {predicted_text}\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to capture test image from the camera\n",
    "capture_test_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3f0c4-4837-41ce-aaee-ecfdb2589dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
